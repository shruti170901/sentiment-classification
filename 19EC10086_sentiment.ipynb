{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# ASSIGNMENT 2\n# SENTIMENT CLASSIFICATION\n## SHRUTI SHREYASI (19EC10086)","metadata":{}},{"cell_type":"code","source":"#necessary imports\nimport numpy as np\nimport pandas as pd\nimport os\nimport torch\nfrom torch.autograd import Variable\nfrom collections import Counter\nimport argparse\nfrom torch import nn, optim\nfrom torch.utils.data import DataLoader\nfrom sklearn.model_selection import train_test_split\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\nimport nltk\nfrom nltk.corpus import stopwords\nprint(stopwords.words('english'))\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\nfrom nltk.tokenize import RegexpTokenizer","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# loading the dataset\ndf = pd.read_csv('/kaggle/input/imdb-dataset/IMDB Dataset.csv')\nprint(df.head())\nprint(df.tail())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# removing the stopwords\nstop_words = set(stopwords.words('english'))\nfor i in range(len(df['review'])):\n    word_tokens = word_tokenize(df['review'][i])\n    filtered_sentence = [w for w in word_tokens if not w.lower() in stop_words]\n    filtered_sentence = []\n    for w in word_tokens:\n        if w not in stop_words:\n            filtered_sentence.append(w)\n    df['review'][i] = filtered_sentence","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# removing non english words\nwords = set(nltk.corpus.words.words())\nfor i in range(len(df['review'])):\n    sent = ''\n    for word in df['review'][i]:\n        sent += word + ' '\n    sent = sent[:len(sent)-1]\n    sent = \" \".join(w for w in nltk.wordpunct_tokenize(sent) \\\n         if w.lower() in words or not w.isalpha())\n    df['review'][i] = sent\ndf.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# removing punctuations\ntokenizer = nltk.RegexpTokenizer(r\"\\w+\")\nfor i in range(len(df['review'])):\n    df['review'][i] = tokenizer.tokenize(df['review'][i])\n    sent = ''\n    for word in df['review'][i]:\n        sent += word + ' '\n    sent = sent[:len(sent)-1]\n    df['review'][i] = sent\ndf.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# loading GloVe embeddings\nvocab,embeddings = [],[]\nwith open('/kaggle/input/glove6b/glove.6B.50d.txt','rt') as fi:\n    full_content = fi.read().strip().split('\\n')\nfor i in range(len(full_content)):\n    i_word = full_content[i].split(' ')[0]\n    i_embeddings = [float(val) for val in full_content[i].split(' ')[1:]]\n    vocab.append(i_word)\n    embeddings.append(i_embeddings)\n    \nvocab_glove = np.array(vocab)\nembs_glove = np.array(embeddings)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# hash of GloVe vocabulary\nvocab_G = {}\nfor i in range(len(vocab_glove)):\n    if vocab_glove[i,] not in vocab_G:\n        vocab_G[vocab_glove[i,]] = embs_glove[i]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# hash of review vocabulary\nvocab = {}\nfor sent in df['review']:\n    for word in sent.split():\n        if word not in vocab:\n            vocab[word] = 1","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# hash of vocabulary of words present in GloVe\nvocab_simplified = {}\nfor word in vocab:\n    if vocab_G.get(word,0) is not 0:\n        vocab_simplified[word] = 1\nprint(len(vocab_simplified))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# length of reviews considered\nLEN = 200\n\n# removing words not present in GloVe hash\nfor i in range(len(df)):\n    sent = ''\n    count = 0\n    for word in df['review'][i].split():\n        if count < LEN and vocab_simplified.get(word,0) is not 0:\n            sent += word + ' '\n            count += 1\n    sent = sent[:len(sent)-1]\n    df['review'][i] = sent\ndf.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# padding reviews to obtain same length\nfor i in range(len(df)):\n    df['review'][i] = ((LEN - len(df['review'][i].split())) * '<PAD> ') + df['review'][i]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# preprocessed dataframe\ndf_preprocessed = df\n\n# storing GloVe embeddings in hash form\nembeddings_glove_complete = {}\nwith open('/kaggle/input/glove6b/glove.6B.50d.txt','rt') as fi:\n    full_content = fi.read().strip().split('\\n')\nfor i in range(len(full_content)):\n    i_word = full_content[i].split(' ')[0]\n    i_embeddings = [float(val) for val in full_content[i].split(' ')[1:]]\n    embeddings_glove_complete[i_word] = i_embeddings\n    \nvocab_imdb = {}\nfor sentence in df['review']:\n    for word in sentence.split():\n        if word in embeddings_glove_complete:\n            vocab_imdb[word] = embeddings_glove_complete[word]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# dataset class for train set\nclass Dataset(torch.utils.data.Dataset):\n    def __init__(self, args):\n        self.args = args\n        (self.train_df, self.train_label) = self.load_words()\n\n    def load_words(self):\n        '''\n            return train data and train labels\n        '''\n        df = df_preprocessed\n        train_df, val_df = train_test_split(df,test_size=0.20,random_state=0)\n        val_df, test_df = train_test_split(val_df,test_size=0.50,random_state=0)\n        traindata = []\n        trainlabel = []\n        for data in train_df['review']:\n            traindata.append(data)\n        for data in train_df['sentiment']:\n            trainlabel.append(data)\n        return (traindata, trainlabel)\n        \n    \n    def words_indexes(self, index):\n        '''\n            embedding of preprocessed review\n        '''\n        arr = []\n        for i in range(index, index + 1):\n            brr = []\n            for word in self.train_df[i].split():\n                brr.append(embeddings_glove_complete.get(word, [0]*50))\n            arr.append(brr)\n        return arr\n    \n                 \n    def label(self, index):\n        '''\n            return label of the review in binary form\n        '''\n        arr = []\n        for i in range(index, index + 1):\n            if self.train_label[i] == 'positive':\n                arr.append(1)\n            else:\n                arr.append(0)\n        return arr\n\n    def __len__(self):\n        '''\n            for dataloader\n        '''\n        return self.args.batch_size\n\n    def __getitem__(self, index):\n        '''\n            for dataloader\n            return <input, label>\n        '''\n        sentence = self.train_df[index]\n        arr = []\n        for i in range(len(sentence.split())):\n            arr.append(embeddings_glove_complete.get(sentence.split()[i], [0]*50))\n        y = 0\n        if self.train_label[index] == 'positive':\n            y = 1\n        return (\n            (torch.tensor(arr).float()),\n            (torch.tensor([y])),\n        )","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# validation dataset\nclass DatasetVal(torch.utils.data.Dataset):\n    def __init__(self, args):\n        self.args = args\n        (self.val_df, self.val_label) = self.load_words()\n\n    def load_words(self):\n        df = df_preprocessed\n        train_df, val_df = train_test_split(df,test_size=0.20,random_state=0)\n        val_df, test_df = train_test_split(val_df,test_size=0.50,random_state=0)\n        valdata = []\n        vallabel = []\n        for data in val_df['review']:\n            valdata.append(data)\n        for data in val_df['sentiment']:\n            vallabel.append(data)\n        return (valdata, vallabel)\n        \n    \n    def words_indexes(self, index):\n        arr = []\n        for i in range(index, index + 1):\n            brr = []\n            for word in self.val_df[i].split():\n                brr.append(embeddings_glove_complete.get(word, [0]*50))\n            arr.append(brr)\n        return arr\n    \n                 \n    def label(self, index):\n        arr = []\n        for i in range(index, index + 1):\n            if self.val_label[i] == 'positive':\n                arr.append(1)\n            else:\n                arr.append(0)\n        return arr\n\n    def __len__(self):\n        return self.args.batch_size\n\n    def __getitem__(self, index):\n        sentence = self.val_df[index]\n        arr = []\n        for i in range(len(sentence.split())):\n            arr.append(embeddings_glove_complete.get(sentence.split()[i], [0]*50))\n        y = 0\n        if self.val_label[index] == 'positive':\n            y = 1\n        return (\n            (torch.tensor(arr).float()),\n            (torch.tensor([y])),\n        )","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# class for model\nclass Model(nn.Module):\n    def __init__(self, dataset):\n        '''\n            Bi-LSTM dimension: 50, 1 layer, 25 hidden dimensions\n            Linear layer dimension: 2*hidden dimensions\n        '''\n        super(Model, self).__init__()\n        self.embedding_dim = 50\n        self.num_layers = 1\n        self.hidden_dim = 100\n\n        self.lstm = nn.LSTM(\n            input_size=self.embedding_dim,\n            hidden_size=self.hidden_dim,\n            num_layers=self.num_layers,\n            batch_first=True,\n            bidirectional=True,\n        )\n        self.fc = nn.Linear(self.hidden_dim*2, 2)\n\n    def forward(self, x, prev_state):\n        output, state = self.lstm(x, prev_state)\n        return self.fc(output.transpose(0,1)[-1]), state\n\n    def init_state(self, batch_size):\n        return (Variable(torch.zeros(self.num_layers*2,batch_size, self.hidden_dim)),\n                Variable(torch.zeros(self.num_layers*2,batch_size, self.hidden_dim)))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# training function\ndef train(dataset, model, args):\n    model.train()\n    \n    dataloader = DataLoader(dataset, batch_size=int(args.batch_size))\n    criterion = nn.CrossEntropyLoss()\n    optimizer = optim.Adam(model.parameters(), lr=0.002)\n    sigmoid = torch.nn.Sigmoid()\n\n    for epoch in range(int(args.max_epochs)):\n        state_h, state_c = model.init_state(int(args.batch_size))\n        state_h = state_h.cuda()\n        state_c = state_c.cuda()\n        \n        # to calculate F1 score\n        tp = 0\n        fp = 0\n        tn = 0\n        fn = 0\n        \n        count = 0\n        accu = 0\n        \n        for batch, (x, y) in enumerate(dataloader):\n            optimizer.zero_grad()\n            x = x.cuda()\n            y_pred, (state_h, state_c) = model(x, (state_h, state_c))\n            state_h = state_h.cuda()\n            state_c = state_c.cuda()\n            y_pred = y_pred.cuda()\n            y = y.cuda()\n            loss = criterion(y_pred, y.flatten())\n            #y_pred is predicted label\n            \n            for i in range(len(y.flatten())):\n                actual = y[i].item()\n                predicted = torch.argmax(y_pred[i]).item()\n    \n                if actual is 1 and predicted is 1:\n                    tp += 1\n                elif actual is 1 and predicted is 0:\n                    fn += 1\n                elif actual is 0 and predicted is 1:\n                    fp += 1\n                else:\n                    tn += 1\n            \n            state_h = state_h.detach()\n            state_c = state_c.detach()\n            loss.backward()\n            optimizer.step()\n            count += 1\n            \n        print({ 'epoch': epoch, 'loss': loss.item(), 'tp':tp, 'tn':tn, 'fp':fp, 'fn':fn})\n        \n        # validation\n        dataset_val = DatasetVal(args)\n        test(dataset_val, model, args)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# args\nparser = argparse.ArgumentParser()\nparser.add_argument('--batch-size', '--max-epochs', '--sequence-length')\nargs = parser.parse_args(['--batch-size', '200', '--max-epochs', '20', '--sequence-length', '200'])\nargs.batch_size = 200\nargs.max_epochs = 13\nargs.sequence_length = 100","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# test dataset\nclass DatasetTest(torch.utils.data.Dataset):\n    def __init__(self, args):\n        self.args = args\n        (self.test_df, self.test_label) = self.load_words()\n\n    def load_words(self):\n        df = df_preprocessed\n        train_df, val_df = train_test_split(df,test_size=0.20,random_state=0)\n        val_df, test_df = train_test_split(val_df,test_size=0.50,random_state=0)\n        testdata = []\n        testlabel = []\n        for data in test_df['review']:\n            testdata.append(data)\n        for data in test_df['sentiment']:\n            testlabel.append(data)\n        return (testdata, testlabel)\n        \n    \n    def words_indexes(self, index):\n        arr = []\n        for i in range(index, index + 1):\n            brr = []\n            for word in self.test_df[i].split():\n                brr.append(embeddings_glove_complete.get(word, [0]*50))\n            arr.append(brr)\n        return arr\n    \n                 \n    def label(self, index):\n        arr = []\n        for i in range(index, index + 1):\n            if self.test_label[i] == 'positive':\n                arr.append(1)\n            else:\n                arr.append(0)\n        return arr\n\n    def __len__(self):\n        return self.args.batch_size\n\n    def __getitem__(self, index):\n        sentence = self.test_df[index]\n        arr = []\n        for i in range(len(sentence.split())):\n            arr.append(embeddings_glove_complete.get(sentence.split()[i], [0]*50))\n        y = 0\n        if self.test_label[index] == 'positive':\n            y = 1\n        return (\n            (torch.tensor(arr).float()),\n            (torch.tensor([y])),\n        )","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def test(dataset, model, args):\n    dataloader = DataLoader(dataset, batch_size=int(args.batch_size))\n    criterion = nn.CrossEntropyLoss()\n    sigmoid = torch.nn.Sigmoid()\n    state_h, state_c = model.init_state(int(args.batch_size))\n    state_h = state_h.cuda()\n    state_c = state_c.cuda()\n        \n    tp = 0\n    fp = 0\n    tn = 0\n    fn = 0\n        \n    count = 0\n    accu = 0\n        \n    for batch, (x, y) in enumerate(dataloader):\n        x = x.cuda()\n        with torch.no_grad():\n            y_pred, (state_h, state_c) = model(x, (state_h, state_c))\n            y_pred = y_pred.cuda()\n            y = y.cuda()\n            loss = criterion(y_pred, y.flatten())\n            \n            for i in range(len(y.flatten())):\n                actual = y[i].item()\n                predicted = torch.argmax(y_pred[i]).item()\n    \n                if actual is 1 and predicted is 1:\n                    tp += 1\n                elif actual is 1 and predicted is 0:\n                    fn += 1\n                elif actual is 0 and predicted is 1:\n                    fp += 1\n                else:\n                    tn += 1\n            print({'tp':tp, 'tn':tn, 'fp':fp, 'fn':fn, 'F1':(2*tp/(2*tp+fp+fn))})\n            count += 1   ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset = Dataset(args)\nmodel = Model(dataset).cuda()\n# comment the below line to use pretrained model\ntrain(dataset, model, args)\n# uncomment the below line to train the model\n# model.load_state_dict(torch.load('/kaggle/input/model.txt'))\ntorch.save(model.state_dict(), '/kaggle/working/model.txt')\ndataset_test = DatasetTest(args)\ntest(dataset_test, model, args)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.load_state_dict(torch.load('/kaggle/working/model.txt'))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset_test = DatasetTest(args)\ntest(dataset_test, model, args)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}